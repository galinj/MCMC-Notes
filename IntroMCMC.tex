\documentclass[12pt]{article}

\usepackage{geometry}
\geometry{hmargin=.75in,vmargin={1.25in,1.25in},nohead,footskip=0.5in}
\renewcommand{\baselinestretch}{1.5}
\setlength{\baselineskip}{0.4in} \setlength{\parskip}{.05in}

%% FIGURES AND TABLES
\usepackage{graphicx}
\usepackage{floatrow}

\usepackage[numbers]{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{verbatim}
\usepackage{subfig}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}{Corollary}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{assmp}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{hw}{Exercise}[section]

\newcommand{\argmin}{\operatorname{argmin}}

\newcommand{\df}{\mathrm{d}}
\newcommand{\ds}{\displaystyle}

\newcommand{\mF}{\mathcal{F}}

\newcommand{\X}{\mathsf{X}}
\newcommand{\Y}{\mathsf{Y}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\BX}{{\cal B}(\mathsf{X})}

\newcommand{\real}{\mathbb{R}}
\newcommand{\exreal}{\overline{\real}}
\newcommand{\tr}{\mbox{tr}}



\title{Markov Chain Monte Carlo}
\author{Galin L. Jones\\
%{}\\
{\small School of Statistics}\\
{\small University of Minnesota}}
\date{Draft: \today}
\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
\label{mcmc:sec:intro}

Later.

\section{Markov Chains}
\label{mcmc:sec:markov}

Let $(\X, \mathcal{B})$ be a measurable space.  A sequence of
$\X$-valued random variables $\{X_1, X_2, X_3, \ldots \}$ is a Markov
chain if for all $g$
\[
E\left[ g(X_{n+1}, X_{n+2}, \ldots) \mid X_n, X_{n-1}, \ldots, X_1
\right] = E\left[ g(X_{n+1}, X_{n+2}, \ldots) \mid X_n\right].
\]

Then $P$ is a {\em Markov kernel}
if $P : \X \times \mathcal{B} \to \real$ satisfying (i) for each fixed
$x \in \X$, $P(x, \cdot)$ is a probability measure and (ii) for each fixed
$B \in \mathcal{B}$, $P(\cdot, B)$ is a measurable function.  

When $\X$ is a discrete set a Markov kernel can be represented as a
square matrix whose entries are nonnegative and whose rows sum to 1.

\begin{example}
  Suppose
  \[
    P= \begin{pmatrix}
      1/2 & 1/2 \\
      1/3 & 2/3\\
      \end{pmatrix}
  \]
  Then $P$ is  a Markov matrix on two states $\{0, 1\}$, say.  The
  first row for example, is interpreted as
  the probability of moving in one step from state 0 to state 0 is 1/2
  which is the same as the probability of moving in one step from state 0 to state 1.
 \end{example}

\begin{example}
  Let $\X=\mathbb{Z}$ and let $0 < \theta < 1$.  If $x \ge 1$, then a
  Markov kernel is defined by the matrix $P$ with elements
  \[
    P(x, x+1) = P(-x, -x-1) = \theta, \quad \quad P(x,0)=P(-x,0) = 1- \theta,
  \]
  and $P(0,1)=P(1,0) =1/2$.
\end{example}

Often $\X$ will be uncountable and $\B$ will be countably
generated.  If $\X$ is topological, then $\B$ will be the Borel
$\sigma$-algebra generated by $\X$.

\begin{example}
  \label{mcmc:ex:betawalk}
 Let $\X  = (0,1)$ and consider the Markov chain that evolves as
 follows. Draw $U \sim \text{Uniform}(0,1)$.  If $u \le 0.$5, $X_{n+1}
 \sim \text{Uniform}(0, X_n)$, but if $u > 0.5$, $X_{n+1}
 \sim \text{Uniform}(X_n, 1)$.  Then if $X_n = x$ and $B \in \B$
 \[
P(x, B) = \int_{B} \left[ \frac{1}{2} \frac{1}{x} I_{y}((0,x)) +
  \frac{1}{2} \frac{1}{1-x} I_{y}(x,1) \right] \df y .
 \]
\end{example}

In Example~\ref{mcmc:ex:betawalk}, the integrand in the Markov kernel
is a conditional density on $\X$.  This is a setting that will be
encountered repeatedly throughout.  If there is a conditional density
$k (y \mid x)$ such that the Markov kernel satisfies for $B \in \B$
\[
P(x, B) = \int_{B} k(y \mid x) \df y
\]
then say $k$ is a {\em Markov transition density}.

\begin{example}
  \label{mcmc:ex:2varGS}
Suppose $f(x,y)$ is a joint density with support $\mathbb{R}^2$ and
conditional densities $f_{X \mid Y}(x \mid y)$ and $f_{Y \mid X}(y
\mid x)$.  Then
\[
  k(x', y' \mid x, y) = f_{X \mid Y}(x' \mid y) f_{Y \mid X}(y' \mid x')
\]
is a Markov transition density.  The Markov chain evolves from
$(X_k = x, Y_k =y)$ to $(X_{k+1}, Y_{k+1})$ by drawing
$X_{k+1} \sim F_{X \mid Y} (\cdot \mid y)$ followed by
$Y_{k+1} \sim F_{Y \mid X}(\cdot \mid X_{k+1})$.  This is a special
case of the so-called two-variable Gibbs sampler.
\end{example}


Suppose $\lambda$ is a positive measure on $(\X, \B)$,  define
\begin{equation}
  \label{mcmc:eq:left}
\lambda P(B) = \int_{\X} \lambda(\df x) P(x, B) .
\end{equation}
When $\lambda$ is a probability measure, the encouraged interpretation
is that $X_{n+1} \mid X_{n} \sim P(X_{n}, \cdot)$ and
$X_{n} \sim \lambda$, the product $\lambda(\df x) P(x, \cdot)$ is the
joint distribution of $(X_n, X_{n+1})$ and $\lambda P$ is the marginal
distribution of $X_{n+1}$.

Since Markov kernels act to the left on measures~\eqref{mcmc:eq:left},
\[
P^2(x, B) = \int_{\X} P(x, \df x_k) P(x_k, B) .
\]
Continuing in this fashion obtain for every $n \ge 2$
\[
P^n(x, B) \int_{\X} P(x, \df x_k) P(x_k, \df x_{k+1}) \cdots P(x_{k +
  n -2}, B) .
\]
More generally, the so-called Chapman-Kolmogorov equations hold for $n
\ge m \ge 0$
\[
P^n(x, B) \int_{\X} P^m(x, \df y) P^{n-m} (y, B) .
\]

If $\lambda = \lambda P$, then $\lambda$ is {\em invariant} for $P$.
Notice that if $\lambda$ is invariant for $P$ and $X_n \sim \lambda$,
then $X_{n+1} \sim \lambda$.  That is, the marginal distribution does
not depend upon $n$ in which case the Markov chain is {\em
  stationary}.

\begin{example}
  \end{example}

\begin{example}
  Recall the Markov chain defined in Example~\eqref{mcmc:ex:2varGS}
\end{example}


One common way of establishing invariance of MCMC Markov chains is to
verify a {\em detailed balance condition}; see
Exercise~\ref{mcmc:hw:dbc}.  Detailed balance holds if
\begin{equation}
   \label{mcmc:eq:dbc}
   \lambda(\df x) P(x, \df y) = \lambda(\df y) P(y, \df x). 
 \end{equation}
 When $\lambda$ is a probability measure, one interpretation is that
 the joint distribution of $(X_k, X_{k+1})$ is the same as the
 distribution of $(X_{k+1}, X_{k})$ so that this is also often called
 the {\em reversibility condition}.  Another name often encountered is
 that $P$ is {\em $\lambda$-symmetric}.

 \subsection{Stability}
 \label{mcmc:sec:stability}
MCMC applications typically are constructed so that a specific probability
 distribution $F$ is invariant.  However, in applications where MCMC is
 required it is typically difficult to simulate from the invariant
 distribution.  The most that can be hoped for is that the simulation
 will eventually produce a representative sample from $F$.  This
 long-run behavior is in no way guaranteed without additional assumptions.

 \begin{example}
   \label{mcmc:ex:detmc}
   Suppose $F$ lives on $\{ 1, 2\}$ with $F(1) = 1 - F(2) = 1/4$ and
   \[
     P = \begin{pmatrix}
       0 & 1 \\
       1 & 0 \\
       \end{pmatrix}
   \]
   Since the Markov chain moves deterministically between the two
   states, it will over represent state 1 and underrepresent state 2
   no matter how many iterations there are.
 \end{example}
 


\section{Metropolis-Hastings}
\label{mcmc:sec:mh}


\section*{Exercises}
\addcontentsline{toc}{section}{Exercises}
\begin{hw}
  \label{mcmc:hw:dbc}
Prove that if Equation~\ref{mcmc:eq:dbc} holds, then $\lambda$ is
invariant for $P$.
\end{hw}

\begin{hw}
Whta is the invariant distribution of the Markov chain defined in
Example~\ref{mcmc:ex:detmc}?
\end{hw}


\newpage

\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}



\bibliography{../mcref}
\bibliographystyle{apalike}
\end{document}